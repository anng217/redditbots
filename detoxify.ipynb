{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from detoxify import Detoxify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each model takes in either a string or a list of strings\n",
    "results = Detoxify('original').predict('example text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Detoxify('unbiased').predict(['example text 1','example text 2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Detoxify('multilingual').predict(['example text','exemple de texte','texto de ejemplo','testo di esempio','texto de exemplo','örnek metin','пример текста'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to specify the device the model will be allocated on (defaults to cpu), accepts any torch.device input\n",
    "model = Detoxify('original', device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).round(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000138</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.004810</td>\n",
       "      <td>0.030099</td>\n",
       "      <td>0.005532</td>\n",
       "      <td>0.026441</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.001036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.022164</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.017627</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.004267</td>\n",
       "      <td>0.033336</td>\n",
       "      <td>0.005408</td>\n",
       "      <td>0.028724</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.012159</td>\n",
       "      <td>0.001841</td>\n",
       "      <td>0.009405</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.002832</td>\n",
       "      <td>0.021326</td>\n",
       "      <td>0.003134</td>\n",
       "      <td>0.017865</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000535</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>0.026462</td>\n",
       "      <td>0.004030</td>\n",
       "      <td>0.021902</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.000747</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   toxicity  severe_toxicity   obscene  identity_attack    insult    threat  \\\n",
       "0  0.000196         0.000193  0.001263         0.000323  0.000883  0.000138   \n",
       "1  0.000563         0.004810  0.030099         0.005532  0.026441  0.002134   \n",
       "2  0.000628         0.003142  0.022164         0.003467  0.017627  0.001497   \n",
       "3  0.000992         0.004267  0.033336         0.005408  0.028724  0.001951   \n",
       "4  0.000612         0.001548  0.012159         0.001841  0.009405  0.000858   \n",
       "5  0.000591         0.002832  0.021326         0.003134  0.017865  0.001327   \n",
       "6  0.000535         0.003826  0.026462         0.004030  0.021902  0.001632   \n",
       "\n",
       "   sexual_explicit  \n",
       "0         0.000090  \n",
       "1         0.001036  \n",
       "2         0.000616  \n",
       "3         0.000833  \n",
       "4         0.000362  \n",
       "5         0.000616  \n",
       "6         0.000747  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0005879871428571428"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get Toxicity Mean\n",
    "results_df['toxicity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "toxicity           0.000588\n",
       "severe_toxicity    0.002945\n",
       "obscene            0.020973\n",
       "identity_attack    0.003391\n",
       "insult             0.017550\n",
       "threat             0.001362\n",
       "sexual_explicit    0.000614\n",
       "dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Get all column Mean\n",
    "results_df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detox_loop(df,model):\n",
    "    detox = Detoxify(model, device='cuda')\n",
    "    i = 0\n",
    "    n = len(df)\n",
    "    df_res = pd.DataFrame()\n",
    "    while i < n:\n",
    "        res = detox.predict(df[i:i+100])\n",
    "        f = pd.DataFrame(res,df[i:i+100]).round(5)\n",
    "        df_res = pd.concat([df_res,f])\n",
    "        i = i + 100\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_base(df_res,thresh):\n",
    "    [float(i) for i in thresh]\n",
    "    df_res['toxicity_flag'] = np.where(df_res['toxicity']>thresh[0],1,0)\n",
    "    df_res['severe_toxicity_flag'] = np.where(df_res['severe_toxicity']>thresh[1],1,0)\n",
    "    df_res['obscene_flag'] = np.where(df_res['obscene']>thresh[2],1,0)\n",
    "    df_res['threat_flag'] = np.where(df_res['threat']>thresh[3],1,0)\n",
    "    df_res['insult_flag'] = np.where(df_res['insult']>thresh[4],1,0)\n",
    "    df_res['identity_attack_flag'] = np.where(df_res['identity_attack']>thresh[5],1,0)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flag_unbiased(df_res,thresh):\n",
    "    [float(i) for i in thresh]\n",
    "    df_res['toxicity_flag'] = np.where(df_res['toxicity']>thresh[0],1,0)\n",
    "    df_res['severe_toxicity_flag'] = np.where(df_res['severe_toxicity']>thresh[1],1,0)\n",
    "    df_res['obscene_flag'] = np.where(df_res['obscene']>thresh[2],1,0)\n",
    "    df_res['threat_flag'] = np.where(df_res['threat']>thresh[3],1,0)\n",
    "    df_res['insult_flag'] = np.where(df_res['insult']>thresh[4],1,0)\n",
    "    df_res['identity_attack_flag'] = np.where(df_res['identity_attack']>thresh[5],1,0)\n",
    "    df_res['sexual_explicit_flag'] = np.where(df_res['sexual_explicit']>thresh[6],1,0)\n",
    "    return df_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(df,model,thresh):\n",
    "    df=list(df['body'].values.flatten())\n",
    "    res = detox_loop(df=df,model=model)\n",
    "    if model == 'original':\n",
    "        res = flag_base(df_res=res, thresh = thresh)\n",
    "    else:\n",
    "        res = flag_unbiased(df_res=res, thresh = thresh)\n",
    "    df_mean = res.mean()\n",
    "    return res,df_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detox(source_dir,model, thresh):\n",
    "    #keeping separate pre-post list\n",
    "    df = pd.read_csv(source_dir)\n",
    "    df_pre = df[df['post']==0]\n",
    "    df_post = df[df['post']==1]\n",
    " \n",
    "    #pre\n",
    "    pre_res,pre_mean = process(df=df_pre,model=model, thresh=thresh)\n",
    "    print(f'Pre: {pre_mean}')\n",
    "    pre_res = pre_res.reset_index()\n",
    "    pre_res.rename(columns ={'index':'body'})\n",
    "    pre_res = pd.concat(df_pre,pre_res)\n",
    "\n",
    "    #post\n",
    "    post_res,post_mean = process(df=df_post,model=model, thresh=thresh)\n",
    "    print(f'Post: {post_mean}')\n",
    "    post_res = post_res.reset_index()\n",
    "    post_res.rename(columns ={'index':'body'})\n",
    "    post_res = pd.concat(df_post,post_res)\n",
    "\n",
    "    return pre_res,post_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(pre_res,post_res,model):\n",
    "    tox = sts.ttest_ind(pre_res['toxicity'], post_res['toxicity'])\n",
    "    print(f'Toxicity: {tox}')\n",
    "\n",
    "    sev_tox = sts.ttest_ind(pre_res['severe_toxicity'], post_res['severe_toxicity'])\n",
    "    print(f'Severe Toxicity: {sev_tox}')\n",
    "\n",
    "    obscene = sts.ttest_ind(pre_res['obscene'], post_res['obscene'])\n",
    "    print(f'Obscene: {obscene}')\n",
    "\n",
    "    threat = sts.ttest_ind(pre_res['threat'], post_res['threat'])\n",
    "    print(f'Threat: {threat}')\n",
    "\n",
    "    insult = sts.ttest_ind(pre_res['insult'], post_res['insult'])\n",
    "    print(f'Insult: {insult}')\n",
    "\n",
    "    identity = sts.ttest_ind(pre_res['identity_attack'], post_res['identity_attack'])\n",
    "    print(f'Identity Attack: {identity}')\n",
    "\n",
    "    tox_f = sts.ttest_ind(pre_res['toxicity_flag'], post_res['toxicity_flag'])\n",
    "    print(f'Toxicity Flag: {tox_f}')\n",
    "\n",
    "    sev_tox_f = sts.ttest_ind(pre_res['severe_toxicity_flag'], post_res['severe_toxicity_flag'])\n",
    "    print(f'Severe Toxicity Flag: {sev_tox_f}')\n",
    "\n",
    "    obscene_f = sts.ttest_ind(pre_res['obscene_flag'], post_res['obscene_flag'])\n",
    "    print(f'Obscene Flag: {obscene_f}')\n",
    "\n",
    "    threat_f = sts.ttest_ind(pre_res['threat_flag'], post_res['threat_flag'])\n",
    "    print(f'Threat Flag: {threat_f}')\n",
    "\n",
    "    insult_f = sts.ttest_ind(pre_res['insult'], post_res['insult'])\n",
    "    print(f'Insult Flag: {insult_f}')\n",
    "\n",
    "    identity_f = sts.ttest_ind(pre_res['identity_attack_flag'], post_res['identity_attack_flag'])\n",
    "    print(f'Identity Attack Flag: {identity}')\n",
    "    \n",
    "    if model == 'unbiased':\n",
    "        sexual_exp = sts.ttest_ind(pre_res['sexual_explicit'], post_res['sexual_explicit'])\n",
    "        print(f'Sexual Explicit: {sexual_exp}')\n",
    "\n",
    "        sexual_exp_f = sts.ttest_ind(pre_res['sexual_explicit_flag'], post_res['sexual_explicit_flag'])\n",
    "        print(f'Sexual Explicit Flag: {sexual_exp_f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## r/feminism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: toxicity                0.179478\n",
      "severe_toxicity         0.008776\n",
      "obscene                 0.089124\n",
      "threat                  0.004224\n",
      "insult                  0.051553\n",
      "identity_attack         0.018389\n",
      "toxicity_flag           0.162950\n",
      "severe_toxicity_flag    0.001546\n",
      "obscene_flag            0.087437\n",
      "threat_flag             0.002208\n",
      "insult_flag             0.036653\n",
      "identity_attack_flag    0.006403\n",
      "dtype: float64\n",
      "Post: toxicity                0.168009\n",
      "severe_toxicity         0.008474\n",
      "obscene                 0.084894\n",
      "threat                  0.003833\n",
      "insult                  0.051067\n",
      "identity_attack         0.015288\n",
      "toxicity_flag           0.153433\n",
      "severe_toxicity_flag    0.001343\n",
      "obscene_flag            0.079402\n",
      "threat_flag             0.001918\n",
      "insult_flag             0.036824\n",
      "identity_attack_flag    0.005754\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "thresh = [.5,.5,.5,.5,.5,.5]\n",
    "thresh = [float(i) for i in thresh]\n",
    "feminism_base_pre,feminism_base_post = detox(source_dir='./data/Control/feminism_clean.csv', model = 'original',thresh = thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: Ttest_indResult(statistic=1.853600213519358, pvalue=0.06382659004506512)\n",
      "Severe Toxicity: Ttest_indResult(statistic=0.3407265465916189, pvalue=0.7333167999434089)\n",
      "Obscene: Ttest_indResult(statistic=0.8803149343337442, pvalue=0.3787104420976326)\n",
      "Threat: Ttest_indResult(statistic=0.5320321206061094, pvalue=0.5947158825402368)\n",
      "Insult: Ttest_indResult(statistic=0.1527983079947721, pvalue=0.8785604968650568)\n",
      "Identity Attack: Ttest_indResult(statistic=2.1791876273363764, pvalue=0.029341610547347192)\n",
      "Toxicity Flag: Ttest_indResult(statistic=1.2849888515759413, pvalue=0.1988267327967718)\n",
      "Severe Toxicity Flag: Ttest_indResult(statistic=0.26388040511235145, pvalue=0.7918776571224689)\n",
      "Obscene Flag: Ttest_indResult(statistic=1.432761808292858, pvalue=0.15195800980732838)\n",
      "Threat Flag: Ttest_indResult(statistic=0.3154951937407536, pvalue=0.7523924583489356)\n",
      "Insult Flag: Ttest_indResult(statistic=0.1527983079947721, pvalue=0.8785604968650568)\n",
      "Identity Attack Flag: Ttest_indResult(statistic=2.1791876273363764, pvalue=0.029341610547347192)\n"
     ]
    }
   ],
   "source": [
    "compare(feminism_base_pre,feminism_base_post, model = 'original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre: toxicity                0.178182\n",
      "severe_toxicity         0.003853\n",
      "obscene                 0.076309\n",
      "identity_attack         0.032643\n",
      "insult                  0.073435\n",
      "threat                  0.004031\n",
      "sexual_explicit         0.059691\n",
      "toxicity_flag           0.159859\n",
      "severe_toxicity_flag    0.000442\n",
      "obscene_flag            0.078384\n",
      "threat_flag             0.002208\n",
      "insult_flag             0.059837\n",
      "identity_attack_flag    0.014352\n",
      "sexual_explicit_flag    0.052329\n",
      "dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15564\\2487416169.py:12: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only.\n",
      "  pre_res = pd.concat(df_pre,pre_res)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\redditbots\\detoxify.ipynb Cell 26\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=0'>1</a>\u001b[0m thresh \u001b[39m=\u001b[39m [\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m,\u001b[39m.5\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=1'>2</a>\u001b[0m thresh \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(i) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m thresh]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=2'>3</a>\u001b[0m feminism_unb_pre,feminism_unb_post \u001b[39m=\u001b[39m detox(source_dir\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m./data/Control/feminism_clean.csv\u001b[39;49m\u001b[39m'\u001b[39;49m, model \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39munbiased\u001b[39;49m\u001b[39m'\u001b[39;49m,thresh \u001b[39m=\u001b[39;49m thresh)\n",
      "\u001b[1;32me:\\github\\redditbots\\detoxify.ipynb Cell 26\u001b[0m in \u001b[0;36mdetox\u001b[1;34m(source_dir, model, thresh)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=9'>10</a>\u001b[0m pre_res \u001b[39m=\u001b[39m pre_res\u001b[39m.\u001b[39mreset_index()\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=10'>11</a>\u001b[0m pre_res\u001b[39m.\u001b[39mrename(columns \u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m:\u001b[39m'\u001b[39m\u001b[39mbody\u001b[39m\u001b[39m'\u001b[39m})\n\u001b[1;32m---> <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=11'>12</a>\u001b[0m pre_res \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mconcat(df_pre,pre_res)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=13'>14</a>\u001b[0m \u001b[39m#post\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000107?line=14'>15</a>\u001b[0m post_res,post_mean \u001b[39m=\u001b[39m process(df\u001b[39m=\u001b[39mdf_post,model\u001b[39m=\u001b[39mmodel, thresh\u001b[39m=\u001b[39mthresh)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:347\u001b[0m, in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[39m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, allowed_args\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mobjs\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    144\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconcat\u001b[39m(\n\u001b[0;32m    145\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    154\u001b[0m     copy: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    155\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[0;32m    156\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    157\u001b[0m \u001b[39m    Concatenate pandas objects along a particular axis with optional set logic\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[39m    along the other axes.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    345\u001b[0m \u001b[39m    ValueError: Indexes have overlapping values: ['a']\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m     op \u001b[39m=\u001b[39m _Concatenator(\n\u001b[0;32m    348\u001b[0m         objs,\n\u001b[0;32m    349\u001b[0m         axis\u001b[39m=\u001b[39;49maxis,\n\u001b[0;32m    350\u001b[0m         ignore_index\u001b[39m=\u001b[39;49mignore_index,\n\u001b[0;32m    351\u001b[0m         join\u001b[39m=\u001b[39;49mjoin,\n\u001b[0;32m    352\u001b[0m         keys\u001b[39m=\u001b[39;49mkeys,\n\u001b[0;32m    353\u001b[0m         levels\u001b[39m=\u001b[39;49mlevels,\n\u001b[0;32m    354\u001b[0m         names\u001b[39m=\u001b[39;49mnames,\n\u001b[0;32m    355\u001b[0m         verify_integrity\u001b[39m=\u001b[39;49mverify_integrity,\n\u001b[0;32m    356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[0;32m    357\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[0;32m    358\u001b[0m     )\n\u001b[0;32m    360\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\pandas\\core\\reshape\\concat.py:382\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[1;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[0;32m    369\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    370\u001b[0m     objs: Iterable[NDFrame] \u001b[39m|\u001b[39m Mapping[Hashable, NDFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    379\u001b[0m     sort\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    380\u001b[0m ):\n\u001b[0;32m    381\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(objs, (ABCSeries, ABCDataFrame, \u001b[39mstr\u001b[39m)):\n\u001b[1;32m--> 382\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    383\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mfirst argument must be an iterable of pandas \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    384\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mobjects, you passed an object of type \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(objs)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    385\u001b[0m         )\n\u001b[0;32m    387\u001b[0m     \u001b[39mif\u001b[39;00m join \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mouter\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    388\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintersect \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: first argument must be an iterable of pandas objects, you passed an object of type \"DataFrame\""
     ]
    }
   ],
   "source": [
    "thresh = [.5,.5,.5,.5,.5,.5,.5]\n",
    "thresh = [float(i) for i in thresh]\n",
    "feminism_unb_pre,feminism_unb_post = detox(source_dir='./data/Control/feminism_clean.csv', model = 'unbiased',thresh = thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity: Ttest_indResult(statistic=1.0941068948819561, pvalue=0.2739351526047687)\n",
      "Severe Toxicity: Ttest_indResult(statistic=0.7168714888169526, pvalue=0.4734705832910765)\n",
      "Obscene: Ttest_indResult(statistic=0.08581130396441883, pvalue=0.9316181839735099)\n",
      "Threat: Ttest_indResult(statistic=0.08021507466557824, pvalue=0.9360678487879641)\n",
      "Insult: Ttest_indResult(statistic=-1.6159819226597314, pvalue=0.10613060530521833)\n",
      "Identity Attack: Ttest_indResult(statistic=2.8798243483258084, pvalue=0.00398764079906833)\n",
      "Toxicity Flag: Ttest_indResult(statistic=0.11614791806836147, pvalue=0.9075377069099041)\n",
      "Severe Toxicity Flag: Ttest_indResult(statistic=1.5175768201161466, pvalue=0.1291535654654599)\n",
      "Obscene Flag: Ttest_indResult(statistic=-0.11593915209209593, pvalue=0.9077031559342021)\n",
      "Threat Flag: Ttest_indResult(statistic=0.10432851489020296, pvalue=0.9169107976624281)\n",
      "Insult Flag: Ttest_indResult(statistic=-1.6159819226597314, pvalue=0.10613060530521833)\n",
      "Identity Attack Flag: Ttest_indResult(statistic=2.8798243483258084, pvalue=0.00398764079906833)\n",
      "Sexual Explicit: Ttest_indResult(statistic=3.0117833316380156, pvalue=0.002603843344028322)\n",
      "Sexual Explicit Flag: Ttest_indResult(statistic=2.692944881303577, pvalue=0.007094517165801728)\n"
     ]
    }
   ],
   "source": [
    "compare(feminism_unb_pre,feminism_unb_post, model = 'unbiased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This part needs to be in func\n",
    "test = pd.read_csv('./data/Control/feminism_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[test['post']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_body=list(test['body'].values.flatten())[1:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = Detoxify('unbiased',device = 'cuda').predict(test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res = pd.DataFrame(test_res,test_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxicity</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>sexual_explicit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>“buT MeN chOoSe tO wOrK thOsE jERbs”</th>\n",
       "      <td>0.146783</td>\n",
       "      <td>0.000125</td>\n",
       "      <td>0.023427</td>\n",
       "      <td>0.006523</td>\n",
       "      <td>0.031431</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.003193</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Are you my excuse to deem feminists anything other than hot-headed attention seekers lacking in self awareness?</th>\n",
       "      <td>0.106481</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.001348</td>\n",
       "      <td>0.036017</td>\n",
       "      <td>0.024539</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.003825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Then do it. \\n\\n“Everyone I’m leaving!”\\n\\n“Bye”</th>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000194</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Wait ...let me get this straight. There are actually people who believe rape, gun violence, gang activity, and terrorism are strictly male issues? Is this satire? I can't comprehend how anyone would be dumb enough to buy this ideology. Ill take my ban now, thanks.</th>\n",
       "      <td>0.664137</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.002260</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.526946</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.008923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>And?</th>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.000129</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.000009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'd like to refer you to rules 1,2 and 3 of this subreddit. One of them is it's forbidden to promote regressive agenda.</th>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>0.000072</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Again, BDSM and Kinks are not same thing and you don't have the guts to either understand or answer the question. \\n\\nWhat's the most comedic thing is you people don't even understand that equating rape with kinks and BDSM, you people actually are performing kink shaming. And then there are matters of women who got criticised for being a sexual dominatrix by people like you.  People like you who don't know nothing about BDSM, kinks or fetishes dominate online spaces where they use their ill informed notions to shame victims, women and practitioners of kinks themselves when they speak out against rape in real life or porn. \"Oh, you hate rape porn, you must be a kink shamer\"!!!</th>\n",
       "      <td>0.748611</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.069258</td>\n",
       "      <td>0.053675</td>\n",
       "      <td>0.178638</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.632282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>I'd like to refer you to rules 4 and 5 of this subreddit.</th>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000092</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>belly breaths! 5 second inhale... 5 second exhale...</th>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.000003</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.000102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>This seems like an unlikely case compared to the usual \"it was consentual\"-defence and a relatively minor risk compared to the improvement on communication and consent bdsm activity can bring.</th>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>0.000116</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    toxicity  severe_toxicity  \\\n",
       "“buT MeN chOoSe tO wOrK thOsE jERbs”                0.146783         0.000125   \n",
       "Are you my excuse to deem feminists anything ot...  0.106481         0.000040   \n",
       "Then do it. \\n\\n“Everyone I’m leaving!”\\n\\n“Bye”    0.001507         0.000002   \n",
       "Wait ...let me get this straight. There are act...  0.664137         0.000070   \n",
       "And?                                                0.000583         0.000001   \n",
       "...                                                      ...              ...   \n",
       "I'd like to refer you to rules 1,2 and 3 of thi...  0.000379         0.000001   \n",
       "Again, BDSM and Kinks are not same thing and yo...  0.748611         0.002017   \n",
       "I'd like to refer you to rules 4 and 5 of this ...  0.000366         0.000001   \n",
       "belly breaths! 5 second inhale... 5 second exha...  0.001771         0.000003   \n",
       "This seems like an unlikely case compared to th...  0.000528         0.000002   \n",
       "\n",
       "                                                     obscene  identity_attack  \\\n",
       "“buT MeN chOoSe tO wOrK thOsE jERbs”                0.023427         0.006523   \n",
       "Are you my excuse to deem feminists anything ot...  0.001348         0.036017   \n",
       "Then do it. \\n\\n“Everyone I’m leaving!”\\n\\n“Bye”    0.000111         0.000120   \n",
       "Wait ...let me get this straight. There are act...  0.002260         0.003002   \n",
       "And?                                                0.000023         0.000123   \n",
       "...                                                      ...              ...   \n",
       "I'd like to refer you to rules 1,2 and 3 of thi...  0.000024         0.000072   \n",
       "Again, BDSM and Kinks are not same thing and yo...  0.069258         0.053675   \n",
       "I'd like to refer you to rules 4 and 5 of this ...  0.000028         0.000065   \n",
       "belly breaths! 5 second inhale... 5 second exha...  0.000113         0.000209   \n",
       "This seems like an unlikely case compared to th...  0.000027         0.000128   \n",
       "\n",
       "                                                      insult    threat  \\\n",
       "“buT MeN chOoSe tO wOrK thOsE jERbs”                0.031431  0.000591   \n",
       "Are you my excuse to deem feminists anything ot...  0.024539  0.000339   \n",
       "Then do it. \\n\\n“Everyone I’m leaving!”\\n\\n“Bye”    0.000194  0.000062   \n",
       "Wait ...let me get this straight. There are act...  0.526946  0.000362   \n",
       "And?                                                0.000129  0.000020   \n",
       "...                                                      ...       ...   \n",
       "I'd like to refer you to rules 1,2 and 3 of thi...  0.000097  0.000018   \n",
       "Again, BDSM and Kinks are not same thing and yo...  0.178638  0.003149   \n",
       "I'd like to refer you to rules 4 and 5 of this ...  0.000092  0.000019   \n",
       "belly breaths! 5 second inhale... 5 second exha...  0.000443  0.000034   \n",
       "This seems like an unlikely case compared to th...  0.000116  0.000021   \n",
       "\n",
       "                                                    sexual_explicit  \n",
       "“buT MeN chOoSe tO wOrK thOsE jERbs”                       0.003193  \n",
       "Are you my excuse to deem feminists anything ot...         0.003825  \n",
       "Then do it. \\n\\n“Everyone I’m leaving!”\\n\\n“Bye”           0.000029  \n",
       "Wait ...let me get this straight. There are act...         0.008923  \n",
       "And?                                                       0.000009  \n",
       "...                                                             ...  \n",
       "I'd like to refer you to rules 1,2 and 3 of thi...         0.000013  \n",
       "Again, BDSM and Kinks are not same thing and yo...         0.632282  \n",
       "I'd like to refer you to rules 4 and 5 of this ...         0.000013  \n",
       "belly breaths! 5 second inhale... 5 second exha...         0.000102  \n",
       "This seems like an unlikely case compared to th...         0.000017  \n",
       "\n",
       "[99 rows x 7 columns]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "feminism_base_post = feminism_base_post.reset_index()['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Series.rename() got an unexpected keyword argument 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\redditbots\\detoxify.ipynb Cell 35\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000119?line=0'>1</a>\u001b[0m feminism_base_post\u001b[39m.\u001b[39;49mrename(columns \u001b[39m=\u001b[39;49m{\u001b[39m'\u001b[39;49m\u001b[39mindex\u001b[39;49m\u001b[39m'\u001b[39;49m:\u001b[39m'\u001b[39;49m\u001b[39mbody\u001b[39;49m\u001b[39m'\u001b[39;49m})\n",
      "\u001b[1;31mTypeError\u001b[0m: Series.rename() got an unexpected keyword argument 'columns'"
     ]
    }
   ],
   "source": [
    "feminism_base_post.pd.rename(columns ={'index':'body'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze FDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import BEFORE data\n",
    "fds_b4_1mo =  pd.read_csv('./data/fds_comments_before_1mo.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave out all 'removed' and 'deleted' comments\n",
    "fds_b4_1mo_df = fds_b4_1mo[(fds_b4_1mo['body'] != '[deleted]') & (fds_b4_1mo['body'] != '[removed]') & (fds_b4_1mo['author'] != 'AutoModerator')]\n",
    "\n",
    "#Flatten to list of string\n",
    "fds_b4_1mo_body = list(fds_b4_1mo_df['body'].values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import AFTER data\n",
    "fds_aft_1mo =  pd.read_csv('./data/fds_comments_after_1mo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leave out all 'removed' and 'deleted' comments\n",
    "fds_aft_1mo_df = fds_aft_1mo[(fds_aft_1mo['body'] != '[deleted]') & (fds_aft_1mo['body'] != '[removed]') & (fds_aft_1mo['author'] !='AutoModerator')]\n",
    "\n",
    "#Flatten to list of string\n",
    "fds_aft_1mo_body = list(fds_aft_1mo_df['body'].values.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Submission\n",
    "Concerns:\n",
    "- Structure of a submission: title, body (`self_text`), images/video link (`domain`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import SUBMISSION BEFORE\n",
    "fds_b4_1mo_sub = pd.read_csv('./data/fds_subm_before_1mo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1869"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of no content submissions\n",
    "fds_b4_1mo_sub['selftext'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46.760070052539405"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Percentage of no content submissions\n",
    "fds_b4_1mo_sub['selftext'].isnull().sum()/len(fds_b4_1mo_sub['selftext'] != '[removed]')*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1136"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition: domain = i.reddit.it OR imugur AND selftext = \"\"\n",
    "len(fds_b4_1mo_sub[((fds_b4_1mo_sub['domain'] == 'i.redd.it') | (fds_b4_1mo_sub['domain'] == 'i.imgur.com') |  (fds_b4_1mo_sub['domain'] == 'imgur.com'))  & fds_b4_1mo_sub['selftext'].isnull()]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1141"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Condition: domain = i.reddit.it OR imugur. So basically they are the same.\n",
    "len(fds_b4_1mo_sub[(fds_b4_1mo_sub['domain'] == 'i.redd.it') | (fds_b4_1mo_sub['domain'] == 'i.imgur.com') |  (fds_b4_1mo_sub['domain'] == 'imgur.com')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2185"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of image submissions\n",
    "len(fds_b4_1mo_sub[(fds_b4_1mo_sub['domain'] == 'self.FemaleDatingStrategy')]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_b4_1mo_sub_df = fds_b4_1mo_sub[fds_b4_1mo_sub['selftext']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save only submissions has written content and not deleted or removed\n",
    "fds_b4_1mo_sub_test = fds_b4_1mo_sub[(fds_b4_1mo_sub['selftext'].astype(bool)) | (fds_b4_1mo_sub['selftext'] == '[removed]')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set pre-trained model and run on GPU\n",
    "originalmodel = Detoxify('original', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the first batch\n",
    "res = originalmodel.predict(fds_b4_1mo_body[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect with body text\n",
    "pd.DataFrame(res,fds_b4_1mo_body[0:100]).round(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = len(fds_b4_1mo_body)\n",
    "fds_b4_1mo_body_res_base = pd.DataFrame()\n",
    "while i < n:\n",
    "    res = originalmodel.predict(fds_b4_1mo_body[i:i+100])\n",
    "    f = pd.DataFrame(res,fds_b4_1mo_body_res_base[i:i+100]).round(5)\n",
    "    fds_b4_1mo_body_res_base = pd.concat([fds_b4_1mo_body_res_base,f])\n",
    "    i = i + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add flag to each comments\n",
    "fds_b4_1mo_body_res_base['toxicity_flag'] = np.where(fds_b4_1mo_body_res_base['toxicity']>0.5,1,0)\n",
    "fds_b4_1mo_body_res_base['severe_toxicity_flag'] = np.where(fds_b4_1mo_body_res_base['severe_toxicity']>0.01,1,0)\n",
    "fds_b4_1mo_body_res_base['obscene_flag'] = np.where(fds_b4_1mo_body_res_base['obscene']>0.5,1,0)\n",
    "fds_b4_1mo_body_res_base['threat_flag'] = np.where(fds_b4_1mo_body_res_base['threat']>0.5,1,0)\n",
    "fds_b4_1mo_body_res_base['insult_flag'] = np.where(fds_b4_1mo_body_res_base['insult']>0.5,1,0)\n",
    "fds_b4_1mo_body_res_base['identity_attack_flag'] = np.where(fds_b4_1mo_body_res_base['identity_attack']>0.5,1,0)\n",
    "print(fds_b4_1mo_body_res_base.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = len(fds_aft_1mo_body)\n",
    "fds_aft_1mo_body_res_base = pd.DataFrame()\n",
    "while i < n:\n",
    "    res = unbiasedmodel.predict(fds_aft_1mo_body_res_base[i:i+100])\n",
    "    f = pd.DataFrame(res,fds_aft_1mo_body_res_base[i:i+100]).round(5)\n",
    "    fds_aft_1mo_body_res_base = pd.concat([fds_aft_1mo_body_res_base,f])\n",
    "    i = i + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add flag to each comments\n",
    "fds_aft_1mo_body_res_base['toxicity_flag'] = np.where(fds_aft_1mo_body_res_base['toxicity']>0.5,1,0)\n",
    "fds_aft_1mo_body_res_base['severe_toxicity_flag'] = np.where(fds_aft_1mo_body_res_base['severe_toxicity']>0.01,1,0)\n",
    "fds_aft_1mo_body_res_base['obscene_flag'] = np.where(fds_aft_1mo_body_res_base['obscene']>0.5,1,0)\n",
    "fds_aft_1mo_body_res_base['threat_flag'] = np.where(fds_aft_1mo_body_res_base['threat']>0.0035,1,0)\n",
    "fds_aft_1mo_body_res_base['insult_flag'] = np.where(fds_aft_1mo_body_res_base['insult']>0.5,1,0)\n",
    "fds_aft_1mo_body_res_base['identity_attack_flag'] = np.where(fds_aft_1mo_body_res_base['identity_attack']>0.5,1,0)\n",
    "fds_aft_1mo_body_res_base['sexual_explicit_flag'] = np.where(fds_aft_1mo_body_res_base['sexual_explicit']>0.5,1,0)\n",
    "print(fds_aft_1mo_body_res_base.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prelim Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as sts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_aft_1mo_body_res_base['post'] = 1\n",
    "fds_b4_1mo_body_res_base['post'] = 0\n",
    "fds = pd.concat([fds_aft_1mo_body_res_base, fds_b4_1mo_body_res_base])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['toxicity'], fds_aft_1mo_body_res_base['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['severe_toxicity'], fds_aft_1mo_body_res_base['severe_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['obscene'], fds_aft_1mo_body_res_base['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['threat'], fds_aft_1mo_body_res_base['threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['insult'], fds_aft_1mo_body_res_base['insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['identity_attack'], fds_aft_1mo_body_res_base['identity_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['toxicity_flag'], fds_aft_1mo_body_res_base['toxicity_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['severe_toxicity_flag'], fds_aft_1mo_body_res_base['severe_toxicity_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['obscene_flag'], fds_aft_1mo_body_res_base['obscene_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['threat_flag'], fds_aft_1mo_body_res_base['threat_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['insult_flag'], fds_aft_1mo_body_res_base['insult_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['identity_attack_flag'], fds_aft_1mo_body_res_base['identity_attack_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res_base['sexual_explicit_flag'], fds_aft_1mo_body_res_base['sexual_explicit_flag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Model - Unbiased Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set pre-trained model and run o\n",
    "unbiasedmodel = Detoxify('unbiased', device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before 1 month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = len(fds_b4_1mo_body)\n",
    "fds_b4_1mo_body_res = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "while i < n:\n",
    "    res = unbiasedmodel.predict(fds_b4_1mo_body[i:i+100])\n",
    "    f = pd.DataFrame(res,fds_b4_1mo_body[i:i+100]).round(5)\n",
    "    fds_b4_1mo_body_res = pd.concat([fds_b4_1mo_body_res,f])\n",
    "    i = i + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity                0.239328\n",
      "severe_toxicity         0.005412\n",
      "obscene                 0.133117\n",
      "identity_attack         0.016930\n",
      "insult                  0.118515\n",
      "threat                  0.003724\n",
      "sexual_explicit         0.078196\n",
      "toxicity_flag           0.234986\n",
      "severe_toxicity_flag    0.089187\n",
      "obscene_flag            0.141755\n",
      "threat_flag             0.002022\n",
      "insult_flag             0.101393\n",
      "identity_attack_flag    0.005017\n",
      "sexual_explicit_flag    0.070990\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Add flag to each comments\n",
    "fds_b4_1mo_body_res['toxicity_flag'] = np.where(fds_b4_1mo_body_res['toxicity']>0.5,1,0)\n",
    "fds_b4_1mo_body_res['severe_toxicity_flag'] = np.where(fds_b4_1mo_body_res['severe_toxicity']>0.01,1,0)\n",
    "fds_b4_1mo_body_res['obscene_flag'] = np.where(fds_b4_1mo_body_res['obscene']>0.5,1,0)\n",
    "fds_b4_1mo_body_res['threat_flag'] = np.where(fds_b4_1mo_body_res['threat']>0.5,1,0)\n",
    "fds_b4_1mo_body_res['insult_flag'] = np.where(fds_b4_1mo_body_res['insult']>0.5,1,0)\n",
    "fds_b4_1mo_body_res['identity_attack_flag'] = np.where(fds_b4_1mo_body_res['identity_attack']>0.5,1,0)\n",
    "fds_b4_1mo_body_res['sexual_explicit_flag'] = np.where(fds_b4_1mo_body_res['sexual_explicit']>0.5,1,0)\n",
    "print(fds_b4_1mo_body_res.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After 1 month comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "n = len(fds_aft_1mo_body)\n",
    "fds_aft_1mo_body_res = pd.DataFrame()\n",
    "while i < n:\n",
    "    res = unbiasedmodel.predict(fds_aft_1mo_body[i:i+100])\n",
    "    f = pd.DataFrame(res,fds_aft_1mo_body[i:i+100]).round(5)\n",
    "    fds_aft_1mo_body_res = pd.concat([fds_aft_1mo_body_res,f])\n",
    "    i = i + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxicity                0.233624\n",
      "severe_toxicity         0.004703\n",
      "obscene                 0.131320\n",
      "identity_attack         0.013421\n",
      "insult                  0.110362\n",
      "threat                  0.004406\n",
      "sexual_explicit         0.078861\n",
      "toxicity_flag           0.230385\n",
      "severe_toxicity_flag    0.085282\n",
      "obscene_flag            0.137574\n",
      "threat_flag             0.074049\n",
      "insult_flag             0.094226\n",
      "identity_attack_flag    0.003203\n",
      "post                    1.000000\n",
      "sexual_explicit_flag    0.072635\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Add flag to each comments\n",
    "fds_aft_1mo_body_res['toxicity_flag'] = np.where(fds_aft_1mo_body_res['toxicity']>0.5,1,0)\n",
    "fds_aft_1mo_body_res['severe_toxicity_flag'] = np.where(fds_aft_1mo_body_res['severe_toxicity']>0.01,1,0)\n",
    "fds_aft_1mo_body_res['obscene_flag'] = np.where(fds_aft_1mo_body_res['obscene']>0.5,1,0)\n",
    "fds_aft_1mo_body_res['threat_flag'] = np.where(fds_aft_1mo_body_res['threat']>0.0035,1,0)\n",
    "fds_aft_1mo_body_res['insult_flag'] = np.where(fds_aft_1mo_body_res['insult']>0.5,1,0)\n",
    "fds_aft_1mo_body_res['identity_attack_flag'] = np.where(fds_aft_1mo_body_res['identity_attack']>0.5,1,0)\n",
    "fds_aft_1mo_body_res['sexual_explicit_flag'] = np.where(fds_aft_1mo_body_res['sexual_explicit']>0.5,1,0)\n",
    "print(fds_aft_1mo_body_res.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prelim regression - 1 mo before and after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "fds_aft_1mo_body_res['post'] = 1\n",
    "fds_b4_1mo_body_res['post'] = 0\n",
    "fds = pd.concat([fds_aft_1mo_body_res, fds_b4_1mo_body_res])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['toxicity'], fds_b4_1mo_body_res['toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.2859843700151743, pvalue=0.1984564801701852)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['severe_toxicity'], fds_b4_1mo_body_res['severe_toxicity'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.119669542293619, pvalue=0.2628618011816403)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['obscene'], fds_b4_1mo_body_res['obscene'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=1.603121251103093, pvalue=0.10891633381862119)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['threat'], fds_b4_1mo_body_res['threat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-4.466846087339321, pvalue=7.959826618625523e-06)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['insult'], fds_b4_1mo_body_res['insult'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-5.756451450590391, pvalue=8.652672465096693e-09)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['identity_attack'], fds_b4_1mo_body_res['identity_attack'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.29539620657489857, pvalue=0.7676928710610675)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['sexual_explicit'], fds_b4_1mo_body_res['sexual_explicit'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perc Flag Difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.0097791287103937, pvalue=0.3126076562298086)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['toxicity_flag'], fds_b4_1mo_body_res['toxicity_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.2859843700151743, pvalue=0.1984564801701852)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['severe_toxicity_flag'], fds_b4_1mo_body_res['severe_toxicity_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.119669542293619, pvalue=0.2628618011816403)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['obscene_flag'], fds_b4_1mo_body_res['obscene_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=31.529471453723872, pvalue=2.293974688841594e-215)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['threat_flag'], fds_b4_1mo_body_res['threat_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-2.2460639116165053, pvalue=0.02470572853072864)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['insult_flag'], fds_b4_1mo_body_res['insult_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-2.713755782402491, pvalue=0.0066555668333369185)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['identity_attack_flag'], fds_b4_1mo_body_res['identity_attack_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.5894638190458906, pvalue=0.5555537378316449)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sts.ttest_ind(fds_aft_1mo_body_res['sexual_explicit_flag'], fds_b4_1mo_body_res['sexual_explicit_flag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =pd.read_csv('./data/Control/feminism_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_body = list(test[test['post']==1]['body'].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32me:\\github\\redditbots\\detoxify.ipynb Cell 111\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/github/redditbots/detoxify.ipynb#ch0000120?line=0'>1</a>\u001b[0m test_res \u001b[39m=\u001b[39m Detoxify(\u001b[39m'\u001b[39;49m\u001b[39munbiased\u001b[39;49m\u001b[39m'\u001b[39;49m,device \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mpredict(test_body[\u001b[39m0\u001b[39;49m:\u001b[39m100\u001b[39;49m])\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\detoxify\\detoxify.py:115\u001b[0m, in \u001b[0;36mDetoxify.predict\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@torch\u001b[39m\u001b[39m.\u001b[39mno_grad()\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m    114\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39meval()\n\u001b[1;32m--> 115\u001b[0m     inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer(text, return_tensors\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m\"\u001b[39;49m, truncation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, padding\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    116\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)[\u001b[39m0\u001b[39m]\n\u001b[0;32m    117\u001b[0m     scores \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msigmoid(out)\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2495\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2490\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2491\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbatch length of `text`: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text)\u001b[39m}\u001b[39;00m\u001b[39m does not match batch length of `text_pair`:\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2492\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(text_pair)\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2493\u001b[0m         )\n\u001b[0;32m   2494\u001b[0m     batch_text_or_text_pairs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(text, text_pair)) \u001b[39mif\u001b[39;00m text_pair \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m text\n\u001b[1;32m-> 2495\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_encode_plus(\n\u001b[0;32m   2496\u001b[0m         batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2497\u001b[0m         add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2498\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2499\u001b[0m         truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   2500\u001b[0m         max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2501\u001b[0m         stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2502\u001b[0m         is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2503\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2504\u001b[0m         return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2505\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2506\u001b[0m         return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2507\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2508\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2509\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2510\u001b[0m         return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2511\u001b[0m         verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2512\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2513\u001b[0m     )\n\u001b[0;32m   2514\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2515\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencode_plus(\n\u001b[0;32m   2516\u001b[0m         text\u001b[39m=\u001b[39mtext,\n\u001b[0;32m   2517\u001b[0m         text_pair\u001b[39m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2533\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2534\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2686\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2676\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[0;32m   2677\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[0;32m   2678\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m   2679\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2683\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2684\u001b[0m )\n\u001b[1;32m-> 2686\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   2687\u001b[0m     batch_text_or_text_pairs\u001b[39m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   2688\u001b[0m     add_special_tokens\u001b[39m=\u001b[39madd_special_tokens,\n\u001b[0;32m   2689\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[0;32m   2690\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   2691\u001b[0m     max_length\u001b[39m=\u001b[39mmax_length,\n\u001b[0;32m   2692\u001b[0m     stride\u001b[39m=\u001b[39mstride,\n\u001b[0;32m   2693\u001b[0m     is_split_into_words\u001b[39m=\u001b[39mis_split_into_words,\n\u001b[0;32m   2694\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[0;32m   2695\u001b[0m     return_tensors\u001b[39m=\u001b[39mreturn_tensors,\n\u001b[0;32m   2696\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39mreturn_token_type_ids,\n\u001b[0;32m   2697\u001b[0m     return_attention_mask\u001b[39m=\u001b[39mreturn_attention_mask,\n\u001b[0;32m   2698\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39mreturn_overflowing_tokens,\n\u001b[0;32m   2699\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39mreturn_special_tokens_mask,\n\u001b[0;32m   2700\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39mreturn_offsets_mapping,\n\u001b[0;32m   2701\u001b[0m     return_length\u001b[39m=\u001b[39mreturn_length,\n\u001b[0;32m   2702\u001b[0m     verbose\u001b[39m=\u001b[39mverbose,\n\u001b[0;32m   2703\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[0;32m   2704\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\transformers\\tokenization_utils.py:737\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    734\u001b[0m     second_ids \u001b[39m=\u001b[39m get_input_ids(pair_ids) \u001b[39mif\u001b[39;00m pair_ids \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    735\u001b[0m     input_ids\u001b[39m.\u001b[39mappend((first_ids, second_ids))\n\u001b[1;32m--> 737\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_prepare_for_model(\n\u001b[0;32m    738\u001b[0m     input_ids,\n\u001b[0;32m    739\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[0;32m    740\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[0;32m    741\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[0;32m    742\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    743\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[0;32m    744\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    745\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    746\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[0;32m    747\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[0;32m    748\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[0;32m    749\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[0;32m    750\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[0;32m    751\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[0;32m    752\u001b[0m )\n\u001b[0;32m    754\u001b[0m \u001b[39mreturn\u001b[39;00m BatchEncoding(batch_outputs)\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\transformers\\tokenization_utils.py:809\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._batch_prepare_for_model\u001b[1;34m(self, batch_ids_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_length, verbose)\u001b[0m\n\u001b[0;32m    806\u001b[0m             batch_outputs[key] \u001b[39m=\u001b[39m []\n\u001b[0;32m    807\u001b[0m         batch_outputs[key]\u001b[39m.\u001b[39mappend(value)\n\u001b[1;32m--> 809\u001b[0m batch_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpad(\n\u001b[0;32m    810\u001b[0m     batch_outputs,\n\u001b[0;32m    811\u001b[0m     padding\u001b[39m=\u001b[39;49mpadding_strategy\u001b[39m.\u001b[39;49mvalue,\n\u001b[0;32m    812\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[0;32m    813\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[0;32m    814\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[0;32m    815\u001b[0m )\n\u001b[0;32m    817\u001b[0m batch_outputs \u001b[39m=\u001b[39m BatchEncoding(batch_outputs, tensor_type\u001b[39m=\u001b[39mreturn_tensors)\n\u001b[0;32m    819\u001b[0m \u001b[39mreturn\u001b[39;00m batch_outputs\n",
      "File \u001b[1;32mc:\\Users\\nguye\\anaconda3\\envs\\redditbots\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2813\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   2811\u001b[0m \u001b[39m# The model's main input name, usually `input_ids`, has be passed for padding\u001b[39;00m\n\u001b[0;32m   2812\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m] \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m encoded_inputs:\n\u001b[1;32m-> 2813\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2814\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou should supply an encoding or a list of encodings to this method \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2815\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mthat includes \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m, but you provided \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlist\u001b[39m(encoded_inputs\u001b[39m.\u001b[39mkeys())\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2816\u001b[0m     )\n\u001b[0;32m   2818\u001b[0m required_input \u001b[39m=\u001b[39m encoded_inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_input_names[\u001b[39m0\u001b[39m]]\n\u001b[0;32m   2820\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m required_input:\n",
      "\u001b[1;31mValueError\u001b[0m: You should supply an encoding or a list of encodings to this method that includes input_ids, but you provided []"
     ]
    }
   ],
   "source": [
    "test_res = Detoxify('unbiased',device = 'cuda').predict(test_body[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('redditbots')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b2a55f9055c2400523b349fd3301d5efeb6493fd652ed128c88c237df22f590"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
